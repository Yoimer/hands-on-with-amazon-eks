1) ********************************************************** Create Cluster

cd scripts-by-chapter
./chapter-1.sh

or

cd hands-on-with-amazon-eks/Infrastructure/eksctl/01-initial-cluster
eksctl create cluster -f cluster.yaml

confirm nodes are running by typing:

kubectl get nodes

2) ********************************************************** Install AWS Load Balancer Controller

cd Infrastructure/k8s-tooling/load-balancer-controller

eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=eks-acg --approve

./create-irsa.sh

3) ********************************************************** Install app

cd Infrastructure/k8s-tooling/load-balancer-controller/test
./run.sh

Verify ALB listeners

Wait for the ALB deployment, and from copy the DNS and confirm ngnix runs

execute:
kubectl get pods -n kube-system

confirm pods names like aws-load-balancer-controller-*******

4) Create the ACM certificate
cd Infrastructure/cloudformation/ssl-certificate
./create.sh

Verify the ACM certificate is generated on AWS ACM

5) Add SSL certificate to the ALB
cd Infrastructure/k8s-tooling/load-balancer-controller/test
./run-with-ssl.sh

Verify ALB listeners, now it has HTTP 80 and HTTP 443 listener
HTTP80 will redirect to HTTP 443 automatically

6) Create External DNS

cd Infrastructure/k8s-tooling/external-dns

./create-irsa.sh

Verify the external-dns-*** pod is running by executing

kubectl get pods

In case the pod shows crashloopbackoff error, proceed with the next:

kubectl logs external-dns-*** pod

if it shows a permission error, delete the pod (it shows redeploy and works well)

Check Route53

confirm the sample-app.******.domainname

sample-app.******.domainname should have https and show the ngnix server.

7) Install the Book Store application

Create dynamodb tables

Create development-clients dynamodb

cd /hands-on-with-amazon-eks/clients-api/infra/cloudformation

./create-dynamodb-table.sh development

Create development-inventory

cd /hands-on-with-amazon-eks/inventory-api/infra/cloudformation

./create-dynamodb-table.sh development

cd /hands-on-with-amazon-eks/renting-api/infra/cloudformation

./create-dynamodb-table.sh development

cd /hands-on-with-amazon-eks/resource-api/infra/cloudformation

./create-dynamodb-table.sh development

check dynamodb tables, it should show:

development-clients
development-inventory
development-renting
development-resources

8) Add AmazonDynamoDBFullAccess policy via irsa

service_account_name="dynamodb-service-account"

eksctl create iamserviceaccount --name ${service_account_name} \
    --cluster eks-acg \
    --attach-policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess --approve

9) Delete my-group load balancer and myapp Route53 records (to avoid issues with the development load balancer)

10) Create deployments

clients-api

cd hands-on-with-amazon-eks/clients-api/infra/helm
./create.sh

resources-api

cd hands-on-with-amazon-eks/resource-api/infra/helm
./create.sh

renting-api

cd hands-on-with-amazon-eks/renting-api/infra/helm
./create.sh

inventory-api

cd hands-on-with-amazon-eks/inventory-api/infra/helm
./create.sh

front-end
cd hands-on-with-amazon-eks/front-end/infra/helm
./create.sh

Check Route53 records. The External DNS makes its job

api.604698758192.realhandsonlabs.net
bookstore.604698758192.realhandsonlabs.net
clients-api.604698758192.realhandsonlabs.net
inventory-api.604698758192.realhandsonlabs.net
renting-api.604698758192.realhandsonlabs.net
resource-api.604698758192.realhandsonlabs.net

11) CNI Add-On

Go to EKS, Add-ons, Get more add-ons, Amazon VPC CNI,  get on the override button

cd Infrastructure/k8s-tooling/cni
./setup.sh

Terminate the EC2's associated with EKS and execute kubectl get pods -n development -o wide
Wait for the new nodes and check the new ips were associated

12) Get nodegroups, delete them and add the spot instances ones.

eksctl get nodegroups --cluster eks-acg

Add spot instances

cd hands-on-with-amazon-eks/Infrastructure/eksctl/02-spot-instances

eksctl create nodegroup -f cluster.yaml

To see on the terminal all the nodegroups, execute

eksctl get nodegroups --cluster eks-acg (both are unmanaged which means that you have to create the ALB. Managed groups are taken care by aws including the ALB, but not fargate yet)

On the AWS console you will not be able to see the nodegroups yet

Delete the normal (on demand) pods

eksctl delete nodegroup --cluster eks-acg eks-node-group

Check again all the nodegroup

eksctl get nodegroups --cluster eks-acg

Create managed nodes

cd hands-on-with-amazon-eks/Infrastructure/eksctl/03-managed-nodes

eksctl create nodegroup -f cluster.yaml

Check again all the nodegroup

eksctl get nodegroups --cluster eks-acg

Delete nodegroup with spot instances

eksctl delete nodegroup --cluster eks-acg eks-node-group-spot-instances

13) Install node termination handler for spot instances on EKS

helm repo add eks https://aws.github.io/eks-charts

helm install aws-node-termination-handler --namespace kube-system eks/aws-node-termination-handler

Check aws-node-termination-handler-**** pods

kubectl get pods -n kube-system

14) Fargate (app will go to fargate, kubernetes tooling to spot instances)

Create Fargate Profile

cd hands-on-with-amazon-eks/Infrastructure/eksctl/04-fargate

eksctl create fargateprofile -f cluster.yaml

After completing the fargate profile creation, go to the AWS console, compute and fargate profile

Check the namespace, it should be development

To confirm the fargate pods, execute:

kubectl get pods -n development -o wide 
You won't see fargate node yet
You have to take down all the current pods which were creating on the runtime face
Fargate nodes are in the scheduling face

kubectl get pods -n development | grep Running | awk '{print $1}'

kubectl delete pods -n development `kubectl get pods -n development | grep Running | awk '{print $1}'`

Check all the new Fargate nodes
kubectl get pods -n development -o wide